# Testing Framework

‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Detection System ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö Unit, Integration, Performance ‡πÅ‡∏•‡∏∞ End-to-End

## üéØ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å

### üìã ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
- **Unit Tests**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
- **Integration Tests**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
- **Performance Tests**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
- **End-to-End Tests**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ú‡πà‡∏≤‡∏ô Browser

### üîß ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö
- **pytest**: Framework ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
- **Selenium**: Browser automation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö E2E testing
- **Locust**: Load testing ‡πÅ‡∏•‡∏∞ Performance testing
- **Memory Profiler**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
- **Coverage**: ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### üìä ‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•
- **HTML Reports**: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Interactive
- **JSON Reports**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
- **XML Reports**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö CI/CD systems
- **Performance Metrics**: ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

## üìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

```
20_Testing_Framework/
‚îú‚îÄ‚îÄ test_framework.py          # Framework ‡∏´‡∏•‡∏±‡∏Å
‚îú‚îÄ‚îÄ test_config.py            # ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Configuration
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îú‚îÄ‚îÄ README.md                # ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ
‚îú‚îÄ‚îÄ test_data/               # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
‚îÇ   ‚îú‚îÄ‚îÄ images/             # ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
‚îÇ   ‚îú‚îÄ‚îÄ videos/             # ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/           # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Mock
‚îÇ   ‚îî‚îÄ‚îÄ mocks/              # Mock responses
‚îú‚îÄ‚îÄ test_results/           # ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
‚îÇ   ‚îú‚îÄ‚îÄ reports/           # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô HTML/JSON
‚îÇ   ‚îú‚îÄ‚îÄ screenshots/       # ‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏à‡∏≤‡∏Å E2E tests
‚îÇ   ‚îî‚îÄ‚îÄ logs/              # Log files
‚îî‚îÄ‚îÄ configs/               # ‡πÑ‡∏ü‡∏•‡πå Configuration
    ‚îú‚îÄ‚îÄ test_config.yaml   # Configuration ‡∏´‡∏•‡∏±‡∏Å
    ‚îú‚îÄ‚îÄ ci_config.yaml     # Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CI
    ‚îî‚îÄ‚îÄ perf_config.yaml   # Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Performance
```

## üöÄ ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á

### 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

```bash
pip install -r requirements.txt
```

### 2. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Browser Drivers

```bash
# Chrome Driver (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÇ‡∏î‡∏¢ webdriver-manager)
# ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á
pip install webdriver-manager
```

### 3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment Variables

```bash
# API Configuration
export API_BASE_URL="http://localhost:8000"
export DATABASE_URL="sqlite:///test.db"

# Browser Configuration
export BROWSER_HEADLESS="true"
export BROWSER_TYPE="chrome"

# Performance Configuration
export CONCURRENT_USERS="10"
export TEST_DURATION="300"

# CI Configuration
export CI="true"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CI environments
```

## üìñ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

```python
from test_framework import TestFramework
from test_config import TestConfig, create_default_config

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Configuration
config = create_default_config()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Testing Framework
framework = TestFramework(config)

# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
results = framework.run_all_tests()

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ
print(f"Total Tests: {results['total_tests']}")
print(f"Passed: {results['passed']}")
print(f"Failed: {results['failed']}")
print(f"Success Rate: {results['success_rate']:.2%}")
```

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó

```python
# Unit Tests ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
unit_results = framework.run_unit_tests()

# Integration Tests ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
integration_results = framework.run_integration_tests()

# Performance Tests ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
performance_results = framework.run_performance_tests()

# E2E Tests ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
e2e_results = framework.run_e2e_tests()
```

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô Command Line

```bash
# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
python test_framework.py

# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
pytest tests/ -m unit
pytest tests/ -m integration
pytest tests/ -m performance
pytest tests/ -m e2e

# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö Parallel
pytest tests/ -n auto

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Coverage
pytest tests/ --cov=src --cov-report=html
```

## ‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Configuration

```python
from test_config import TestConfig, TestEnvironment, TestType

# Configuration ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
config = TestConfig(
    environment=TestEnvironment.DEVELOPMENT,
    enabled_test_types=[TestType.UNIT, TestType.INTEGRATION]
)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ API
config.api.base_url = "http://localhost:8000"
config.api.timeout = 30

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Performance Thresholds
config.performance.response_time_ms = 2000
config.performance.memory_usage_mb = 500
config.performance.accuracy_threshold = 0.95

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Load Testing
config.load_test.concurrent_users = 50
config.load_test.test_duration = 600

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Configuration
config.save_to_yaml_file("my_test_config.yaml")
```

### ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Configuration ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå

```python
# ‡∏à‡∏≤‡∏Å YAML
config = TestConfig.from_yaml_file("test_config.yaml")

# ‡∏à‡∏≤‡∏Å JSON
config = TestConfig.from_json_file("test_config.json")

# ‡∏à‡∏≤‡∏Å Environment Variables
config = load_config_from_environment()
```

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Configuration File (YAML)

```yaml
environment: development
enabled_test_types:
  - unit
  - integration
  - performance

api:
  base_url: "http://localhost:8000"
  timeout: 30
  retry_attempts: 3

performance:
  response_time_ms: 2000.0
  memory_usage_mb: 500.0
  cpu_usage_percent: 80.0
  accuracy_threshold: 0.95

load_test:
  concurrent_users: 10
  test_duration: 300
  requests_per_user: 100

browser:
  browser_type: chrome
  headless: true
  window_width: 1920
  window_height: 1080

reporting:
  output_path: "test_results"
  report_format: ["json", "html", "xml"]
  include_screenshots: true
```

## üß™ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Tests

### Unit Test Example

```python
import pytest
from test_framework import UnitTestSuite
from test_config import create_default_config

class TestImageProcessing:
    def setup_method(self):
        self.config = create_default_config()
        self.unit_suite = UnitTestSuite(self.config)
    
    def test_image_resize(self):
        result = self.unit_suite.test_image_preprocessing()
        assert result.status == "passed"
        assert result.metrics["operations_tested"] == 3
    
    def test_model_inference(self):
        result = self.unit_suite.test_model_inference()
        assert result.status == "passed"
        assert "inference_time" in result.metrics
```

### Integration Test Example

```python
import pytest
import requests
from test_framework import IntegrationTestSuite

class TestAPIIntegration:
    def setup_method(self):
        self.config = create_default_config()
        self.integration_suite = IntegrationTestSuite(self.config)
    
    def test_detection_endpoint(self):
        # Test data
        test_data = {
            "image": "base64_encoded_image",
            "confidence_threshold": 0.8
        }
        
        # Make request
        response = requests.post(
            f"{self.config.api.base_url}/detect",
            json=test_data
        )
        
        # Assertions
        assert response.status_code == 200
        result = response.json()
        assert "detections" in result
        assert "confidence" in result
```

### Performance Test Example

```python
import time
import pytest
from test_framework import PerformanceTestSuite

class TestPerformance:
    def setup_method(self):
        self.config = create_default_config()
        self.perf_suite = PerformanceTestSuite(self.config)
    
    def test_response_time_benchmark(self):
        result = self.perf_suite.test_response_time()
        
        # Check if response time meets threshold
        avg_time = result.metrics["avg_response_time"]
        threshold = self.config.performance.response_time_ms / 1000
        
        assert avg_time <= threshold, f"Response time {avg_time}s exceeds threshold {threshold}s"
    
    @pytest.mark.slow
    def test_load_capacity(self):
        result = self.perf_suite.test_concurrent_load()
        
        # Check success rate
        success_rate = result.metrics["success_rate"]
        assert success_rate >= 0.95, f"Success rate {success_rate:.2%} below 95%"
```

### E2E Test Example

```python
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from test_framework import E2ETestSuite

class TestWebInterface:
    def setup_method(self):
        self.config = create_default_config()
        self.e2e_suite = E2ETestSuite(self.config)
    
    def test_image_upload_workflow(self):
        # Setup browser
        self.e2e_suite.setup_browser()
        driver = self.e2e_suite.driver
        
        try:
            # Navigate to app
            driver.get(self.config.api.base_url)
            
            # Upload image
            upload_input = driver.find_element(By.CSS_SELECTOR, "input[type='file']")
            upload_input.send_keys("/path/to/test/image.jpg")
            
            # Click detect button
            detect_button = driver.find_element(By.CSS_SELECTOR, ".detect-button")
            detect_button.click()
            
            # Wait for results
            results = WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".results"))
            )
            
            # Verify results
            assert results.is_displayed()
            
        finally:
            self.e2e_suite.teardown_browser()
```

## üìä Performance Testing

### Load Testing with Locust

```python
from locust import HttpUser, task, between

class DetectionUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """Setup user session"""
        self.test_image = "base64_encoded_test_image"
    
    @task(3)
    def test_health_check(self):
        """Test health endpoint"""
        self.client.get("/health")
    
    @task(1)
    def test_detection(self):
        """Test detection endpoint"""
        self.client.post("/detect", json={
            "image": self.test_image,
            "confidence_threshold": 0.8
        })

# ‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢: locust -f locust_tests.py --host=http://localhost:8000
```

### Memory Profiling

```python
import memory_profiler

@memory_profiler.profile
def test_memory_intensive_operation():
    """Test memory usage during heavy operations"""
    # Simulate heavy image processing
    large_images = []
    for i in range(100):
        image = np.random.rand(1000, 1000, 3)
        processed = process_image(image)
        large_images.append(processed)
    
    return large_images

# ‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢: python -m memory_profiler test_memory.py
```

## üîÑ CI/CD Integration

### GitHub Actions Example

```yaml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run unit tests
      run: |
        pytest tests/ -m unit --cov=src --cov-report=xml
    
    - name: Run integration tests
      run: |
        pytest tests/ -m integration
    
    - name: Upload coverage
      uses: codecov/codecov-action@v1
      with:
        file: ./coverage.xml
```

### Docker Testing Environment

```dockerfile
FROM python:3.9-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    chromium-driver \
    chromium-browser \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy test files
COPY . .

# Set environment variables
ENV BROWSER_HEADLESS=true
ENV CI=true

# Run tests
CMD ["python", "test_framework.py"]
```

## üìà ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

### Performance Monitoring

```python
import psutil
import time
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    cpu_percent: float
    memory_mb: float
    response_time_ms: float
    throughput_rps: float

def monitor_performance(func):
    """Decorator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û"""
    def wrapper(*args, **kwargs):
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
        start_time = time.time()
        start_cpu = psutil.cpu_percent()
        start_memory = psutil.virtual_memory().used / 1024 / 1024
        
        # ‡∏£‡∏±‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
        result = func(*args, **kwargs)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
        end_time = time.time()
        end_cpu = psutil.cpu_percent()
        end_memory = psutil.virtual_memory().used / 1024 / 1024
        
        metrics = PerformanceMetrics(
            cpu_percent=(start_cpu + end_cpu) / 2,
            memory_mb=end_memory - start_memory,
            response_time_ms=(end_time - start_time) * 1000,
            throughput_rps=1 / (end_time - start_time) if end_time > start_time else 0
        )
        
        print(f"Performance Metrics: {metrics}")
        return result
    
    return wrapper

# ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
@monitor_performance
def test_detection_api():
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö API
    pass
```

### Benchmarking

```python
import pytest
import time

class TestBenchmarks:
    def test_image_processing_benchmark(self, benchmark):
        """Benchmark image processing performance"""
        test_image = create_test_image()
        
        # Benchmark the function
        result = benchmark(process_image, test_image)
        
        # Assertions
        assert result is not None
        assert benchmark.stats.mean < 0.1  # Less than 100ms
    
    def test_model_inference_benchmark(self, benchmark):
        """Benchmark model inference performance"""
        test_input = create_test_input()
        
        result = benchmark(model_inference, test_input)
        
        assert result is not None
        assert benchmark.stats.mean < 0.05  # Less than 50ms
```

## üõ°Ô∏è Security Testing

### Security Test Examples

```python
import pytest
import requests
from test_framework import SecurityTestSuite

class TestSecurity:
    def test_sql_injection(self):
        """Test SQL injection vulnerabilities"""
        malicious_inputs = [
            "'; DROP TABLE users; --",
            "1' OR '1'='1",
            "admin'/*",
        ]
        
        for payload in malicious_inputs:
            response = requests.post("/api/search", json={
                "query": payload
            })
            
            # Should not return sensitive data
            assert response.status_code != 200 or "error" in response.json()
    
    def test_xss_protection(self):
        """Test XSS protection"""
        xss_payloads = [
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            "<img src=x onerror=alert('xss')>",
        ]
        
        for payload in xss_payloads:
            response = requests.post("/api/comment", json={
                "content": payload
            })
            
            # Should sanitize input
            if response.status_code == 200:
                assert "<script>" not in response.text
                assert "javascript:" not in response.text
    
    def test_rate_limiting(self):
        """Test rate limiting"""
        # Make rapid requests
        responses = []
        for i in range(100):
            response = requests.get("/api/health")
            responses.append(response.status_code)
        
        # Should have some rate limited responses
        rate_limited = [r for r in responses if r == 429]
        assert len(rate_limited) > 0, "Rate limiting not working"
```

## üìã ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

1. **Browser Driver Issues**
   ```bash
   # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á webdriver-manager
   pip install webdriver-manager
   
   # ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î driver ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á
   from webdriver_manager.chrome import ChromeDriverManager
   driver = webdriver.Chrome(ChromeDriverManager().install())
   ```

2. **Memory Issues**
   ```python
   # ‡πÄ‡∏û‡∏¥‡πà‡∏° memory limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tests
   pytest tests/ --maxfail=1 --tb=short
   
   # ‡πÉ‡∏ä‡πâ garbage collection
   import gc
   gc.collect()
   ```

3. **Timeout Issues**
   ```python
   # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö slow tests
   @pytest.mark.timeout(300)
   def test_slow_operation():
       pass
   ```

4. **Database Connection Issues**
   ```python
   # ‡πÉ‡∏ä‡πâ test database ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å
   TEST_DATABASE_URL = "sqlite:///test.db"
   
   # Cleanup ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
   def teardown_method(self):
       db.session.remove()
       db.drop_all()
   ```

### ‡∏Å‡∏≤‡∏£ Debug Tests

```python
import pytest
import logging

# ‡πÄ‡∏õ‡∏¥‡∏î debug logging
logging.basicConfig(level=logging.DEBUG)

# ‡πÉ‡∏ä‡πâ pytest debugging
pytest tests/ -v -s --tb=long

# ‡πÉ‡∏ä‡πâ pdb ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö interactive debugging
pytest tests/ --pdb

# ‡πÉ‡∏ä‡πâ pytest-xvs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö verbose output
pytest tests/ -xvs
```

## üìä ‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•

### HTML Report Generation

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô HTML
pytest tests/ --html=reports/report.html --self-contained-html

# ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏° screenshots
pytest tests/ --html=reports/report.html --capture=sys
```

### Custom Report Generation

```python
import json
from datetime import datetime

def generate_custom_report(test_results):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total": len(test_results),
            "passed": len([r for r in test_results if r.status == "passed"]),
            "failed": len([r for r in test_results if r.status == "failed"]),
        },
        "details": [
            {
                "name": r.test_name,
                "status": r.status,
                "duration": r.duration,
                "error": r.error_message
            }
            for r in test_results
        ]
    }
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
    with open("custom_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    return report
```

## üöÄ ‡∏Å‡∏≤‡∏£ Deploy ‡πÅ‡∏•‡∏∞ Scaling

### Distributed Testing

```python
# ‡∏£‡∏±‡∏ô tests ‡πÅ‡∏ö‡∏ö parallel
pytest tests/ -n auto  # ‡πÉ‡∏ä‡πâ CPU cores ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
pytest tests/ -n 4     # ‡πÉ‡∏ä‡πâ 4 processes

# ‡∏£‡∏±‡∏ô tests ‡∏ö‡∏ô multiple machines
pytest tests/ --dist=loadscope --tx=ssh://user@host1//python
```

### Cloud Testing

```yaml
# AWS CodeBuild example
version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.9
    commands:
      - pip install -r requirements.txt
  
  pre_build:
    commands:
      - echo Starting tests...
  
  build:
    commands:
      - python test_framework.py
      - pytest tests/ --junitxml=test-results.xml
  
  post_build:
    commands:
      - echo Tests completed

reports:
  test-reports:
    files:
      - test-results.xml
    base-directory: .
```

## üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

- [pytest Documentation](https://docs.pytest.org/)
- [Selenium Documentation](https://selenium-python.readthedocs.io/)
- [Locust Documentation](https://docs.locust.io/)
- [Coverage.py Documentation](https://coverage.readthedocs.io/)

## ü§ù ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°

1. Fork repository
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á feature branch
3. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö features ‡πÉ‡∏´‡∏°‡πà
4. ‡∏£‡∏±‡∏ô test suite ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
5. Submit pull request

## üìÑ License

MIT License - ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå LICENSE

## üìû Support

- Email: support@example.com
- Issues: GitHub Issues
- Documentation: Wiki pages

---

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: Testing Framework ‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà Unit tests ‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á End-to-End tests ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö AI Detection